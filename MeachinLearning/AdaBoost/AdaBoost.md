## AdaBoost 算法
强可学习的充分必要条件是弱可学习。 在学习中如果已经发现‘弱可学习算法’ 那么是否可以提升（boost）为‘强学习算法’？ 最具代表性的是 AdaBoost 算法。
对于一个分类问题而言，给定一个训练样本集，求比较粗糙的分类规律 要比精确地容易的多。 提升方法就是从弱学习算法出发，反复学习得到一系列弱分类器（基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法一系列弱分类器。

这样对于提升算法 需要回答2个问题：
- **在每一轮如何改变训练数据的权重或者概率分布？**
    AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值，使没有得到正确分类的数据由于其权值加大而受到后一轮弱分类器的更大关注。于是分类问题被一系列的弱分类器‘分而治之’。
- **如何将弱分类器组合成一个强分类器？**
    AdaBoost采取加权多数表决法，具体地，加大分类误差概率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较大的作用。

### 算法
输入：训练数据集T = {(x1,y1),(x2,y2),...(xn,yn)},  yi: {-1,+1}; 弱学习算法；
输出： 最终分类器G(x).
**1) 初始化训练数据的权值分布。**

    D1 = (Wl1,W12,....Wln), Wli = 1/N; i = 1,2,...N

假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第一步能在原始数据上学习基本分类器G1(x).

**2) 对m=1,2,...M**
a) 使用具有权值分布Dm 的训练数据集学习，得到基本分类器

            Gm(x): x->{-1, +1}

b) 计算Gm(x) 在训练数据集上的分类误差率em

            em=sum(P(Gm(xi) != yi))

c) 计算Gm(x) 在最终分类器中的系数am

            am = 1/2 * log((1-em)/em) 

am 表示Gm(x)在最终分类器中的重要性，由上公式可知，(1-em)/em >1时，am >=0, 即当em <=1/2时，am >=0,并且em越小，am 越大。am随着em减小而增大，所以误差率越小的基本分类器在最终分类器中的作用越大。

d) 更新训练数据集的权值分布

            Dm+1 = (Wm+1,l， Wm+1,i.....Wm+1,N)
            Wm+1,i = Wmi/Zm exp(-am*yi*Gm(xi))

    这里Zm是规范化因子
        Zm = Sum(Wmi* exp(-am * yi *Gm(xi))) 
    使Dm+1 成为一个概率分布。
更新训练数据的权值分布，为下一轮作准备,可以写成下面形式：

    Wm+1,i = Wmi/Zm * e^(-am),   Gm(xi) = yi
    Wm+1,i = Wmi/Zm * e^(am),    Gm(xi) != yi

由此可知，被基本分类器Gm(x)误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小，两者相比较，误分类样本的权值被放大 e^(2am) = (1-em)/em 倍。因此误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起不同的作用。这是AdaBoost的特点。

**3) 构建基本分类器的线性组合**

        f(x) = sum(am*Gm(x))

最终分类器：

        G(x) = sign(f(x)) = sign(sum(am*Gm(x)))

线性组合f(x)实现了M个基本分类器的加权表决，系数am表示了基本分类器Gm的重要性，这里所有am 之和并不为1. f(x)的符号决定实例x的类， f(x)绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。

## AdaBoost 算法解释

AdaBoost算法是模型为加法模型，损失函数为指数函数，学习算法为前向分布算法时的二分类学习方法。

## 算法步骤

- 获取单层最佳决策树 buildStump()

```py
    只基于单个特征来做决策，只有一次分裂过程，实际上就是一个树桩。
    将错误率minError 设为无穷大
    对数据集中的每一个特征（第一个层循环）
        对每一个步长（第二层循环）
            对每个不等式（第三层循环）
                建立一棵单层决策树 ## 通过某个特征，特征中的某个值 来进行分类 得到分类结果C --->stumpClassify
                并利用加权数据集对他进行测试 ## 基于权重向量D 乘上分类结果误差矩阵（正确0，错误1）来计算错误率。
                如果错误率低于minError，则将当前层决策树设为最佳单层决策树
    返回最佳单层决策树
```

- 基于单层最佳的AdaBoost算法

```py
    初始化权重向量D
    对每次迭代：
        利用buildStump()函数找到最佳的单层决策树
        将最佳单层决策树加入到单层决策树数组中
        计算alpha(该单层决策树的线性系数：根据误差结果来计算 alpha = 1/2 * log((1-e)/e))
        计算新的权重向量D
            expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)
            D = np.multiply(D, np.exp(expon))
            D = D/D.sum()
        更新累计类别估计值
        如果错误率等于0，则退出循环
```

- 利用AdaBoost算法分类
  
```py

    输入测试数据，和AdaBoost 决策数组
    对于决策数组的每一个树桩：
        通过stumpClassify(测试数据，树桩的特征，阈值，符号) 计算分类结果
        计算线性加权 分类结果
    返回 加权结果的分类 (sign(加权结果)：2分类，分类结果为 +1， -1) 

```

